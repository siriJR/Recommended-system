{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run round2_base.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sample = get_user('recall')\n",
    "df = get_hdf(dtype='all', if_filter_label=True, if_drop_duplicates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_prefix(df, exclude_columns, prefix):\n",
    "    if isinstance(exclude_columns, str):\n",
    "        exclude_columns = [exclude_columns]\n",
    "        \n",
    "    column_names = [col for col in df.columns if col not in exclude_columns]\n",
    "    df.rename(columns = dict(zip(column_names, [prefix + name for name in column_names])), inplace=True)\n",
    "    return df\n",
    "\n",
    "def group_func(df, group_func_dic, group_key):\n",
    "    if isinstance(group_func_dic, str):\n",
    "        group_func_dic = [group_func_dic]\n",
    "        \n",
    "    features = df.groupby(group_key).agg(group_func_dic)\n",
    "    features.columns = [e[0] + \"_\" + e[1].upper() for e in features.columns.tolist()]\n",
    "    features.reset_index(inplace=True)\n",
    "    return features\n",
    "\n",
    "def filter_sample(df, key=None):\n",
    "    if key is None:\n",
    "        df = df.merge(sample[['buyer_admin_id']].drop_duplicates(), on=['buyer_admin_id'], how='inner')\n",
    "    else:\n",
    "        df = df.merge(sample[['buyer_admin_id', key]].drop_duplicates(), on=['buyer_admin_id', key], how='inner')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_store_dedup_feature(df):\n",
    "    \"\"\"\n",
    "    商品 * 品类基础特征：\n",
    "    1. 行为数： #TODO: 用户下单数 * 划窗\n",
    "    2. 行为时间：去重，首次，末次，首末差\n",
    "        i: 天\n",
    "        ii: 小时\n",
    "        iii: 秒\n",
    "    3. 店铺数：去重\n",
    "    4. 商品数：去重\n",
    "    5. 商品价格：最大、最小、平均、求和、var、std\n",
    "    6. 用户数：去重\n",
    "    \n",
    "    备注：线下：0.8697→0.8764  提升：0.0067\n",
    "    ---------------------------------------------\n",
    "    \n",
    "    \"\"\"\n",
    "    feature_type = {\n",
    "        'item_id' : ['nunique'],\n",
    "        'cate_id' : ['nunique'],\n",
    "        'second' : ['nunique', 'max', 'min', 'mean', 'std', np.ptp],\n",
    "        'first_second_diff':['max', 'min', 'mean'],\n",
    "        'last_second_diff':['max', 'min', 'mean'],\n",
    "        'day':['nunique', 'max', 'min', np.ptp],\n",
    "        'item_price': ['max', 'min'],\n",
    "        'dense_rank':['max', 'min', 'mean', 'std', np.ptp],\n",
    "    }\n",
    "    df = filter_sample(df)\n",
    "    feature = group_func(df, feature_type, group_key=['buyer_admin_id', 'store_id'])\n",
    "    feature = add_prefix(feature, ['buyer_admin_id', 'store_id'], 'user_store_dedup_')\n",
    "    feature.to_hdf('../feature/recall/user_store_dedup_feature', 'all')\n",
    "    print('>>> user_store_dedup_feature success')\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_cate_dedup_feature(df):\n",
    "    \"\"\"\n",
    "    商品 * 品类基础特征：\n",
    "    1. 行为数： #TODO: 用户下单数 * 划窗\n",
    "    2. 行为时间：去重，首次，末次，首末差\n",
    "        i: 天\n",
    "        ii: 小时\n",
    "        iii: 秒\n",
    "    3. 店铺数：去重\n",
    "    4. 商品数：去重\n",
    "    5. 商品价格：最大、最小、平均、求和、var、std\n",
    "    6. 用户数：去重\n",
    "    \n",
    "    备注：线下：0.8697→0.8764  提升：0.0067\n",
    "    ---------------------------------------------\n",
    "    \n",
    "    \"\"\"\n",
    "    feature_type = {\n",
    "        'item_id' : ['nunique'],\n",
    "        'store_id' : ['nunique'],\n",
    "        'second' : ['nunique', 'max', 'min', 'mean', np.ptp],\n",
    "        'first_second_diff':['max', 'min', 'mean'],\n",
    "        'last_second_diff':['max', 'min', 'mean'], \n",
    "        'day':['nunique', 'max', 'min', np.ptp],\n",
    "        'item_price': ['max', 'min'],\n",
    "        'dense_rank':['max', 'min', 'mean', np.ptp],\n",
    "    }\n",
    "    df = filter_sample(df)\n",
    "    feature = group_func(df, feature_type, group_key=['buyer_admin_id', 'cate_id'])\n",
    "    feature = add_prefix(feature, ['buyer_admin_id', 'cate_id'], 'user_cate_dedup_')\n",
    "    feature.to_hdf('../feature/recall/user_cate_dedup_feature', 'all')\n",
    "    print('>>> user_cate_dedup_feature feature success')\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_item_feature(df, name='all'):\n",
    "    \"\"\"\n",
    "    df = get_hdf(dtype='buy', if_filter_label=True, if_drop_duplicates=True)\n",
    "    get_item_feature(df, name='buy')\n",
    "    \n",
    "    商品基础特征：\n",
    "    1. 行为数： #TODO: 用户下单数 * 划窗\n",
    "    2. 行为时间：去重，首次，末次，首末差\n",
    "        i: 天\n",
    "        ii: 小时\n",
    "        iii: 秒\n",
    "    3. 品类数：去重\n",
    "    6. 商品价格：最大、最小、平均、求和、var、std\n",
    "    3. 用户数：去重\n",
    "    \n",
    "    备注：线下：0.8795→0.8795  提升：0\n",
    "    ---------------------------------------------\n",
    "    \"\"\"\n",
    "    feature_type = {\n",
    "        'item_id' : ['count'],\n",
    "        'buyer_admin_id' : ['nunique'],\n",
    "        'day': ['max', 'min', 'nunique'],\n",
    "        'second' : ['max', 'min', 'nunique', np.ptp],\n",
    "    }\n",
    "    feature = group_func(df, feature_type, group_key=['item_id'])\n",
    "    feature = add_prefix(feature, ['item_id'], 'item_' + name +'_')\n",
    "    \n",
    "    feature.to_hdf('../feature/recall/item_feature', name)\n",
    "    print('>>> item feature success')\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cate_feature(df, name='all'):\n",
    "    \"\"\"\n",
    "    品类基础特征：\n",
    "    1. 行为数\n",
    "    2. 行为时间：去重，首次，末次，首末差\n",
    "        i: 天\n",
    "        iii: 秒\n",
    "    3. 用户数：去重\n",
    "    4. 商品数：去重\n",
    "    5. 店铺数：去重\n",
    "    6. 商品价格：最大、最小、差值\n",
    "    \n",
    "    备注：线下：0.8795→0.8795  提升：0\n",
    "    ---------------------------------------------\n",
    "    \"\"\"\n",
    "    feature_type = {\n",
    "        'cate_id' : ['count'],\n",
    "        'buyer_admin_id' : ['nunique'],\n",
    "        'item_id' :['nunique'],\n",
    "        'store_id' : ['nunique'],\n",
    "        'item_price': ['min', 'max', np.ptp],\n",
    "        'day': ['max', 'min', 'nunique'],\n",
    "        'second' : ['max', 'min', 'nunique', np.ptp],\n",
    "    }\n",
    "\n",
    "    feature = group_func(df, feature_type, group_key=['cate_id'])\n",
    "    feature = add_prefix(feature, ['cate_id'], 'cate_' + name + '_')\n",
    "    feature.to_hdf('../feature/recall/cate_feature', name)\n",
    "    print('>>> cate feature success')\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_store_feature(df, name='all'):\n",
    "    \"\"\"\n",
    "    店铺基础特征：\n",
    "    1. 行为数： #TODO: 用户下单数 * 划窗\n",
    "    2. 行为时间：去重，首次，末次，首末差\n",
    "        i: 天\n",
    "        iii: 秒\n",
    "    3. 用户数：去重\n",
    "    4. 商品数：去重\n",
    "    5. 品类数：去重\n",
    "    6. 商品价格：最大、最小、差值\n",
    "    \n",
    "    备注：线下：0.8795→0.8795  提升：0\n",
    "    ---------------------------------------------\n",
    "    \n",
    "    \"\"\"\n",
    "    feature_type = {\n",
    "        'store_id' : ['count'],\n",
    "        'buyer_admin_id' : ['nunique'],\n",
    "        'item_id' :['nunique'],\n",
    "        'cate_id' : ['nunique'],\n",
    "        'item_price': ['min', 'max', np.ptp],\n",
    "        'day': ['max', 'min', 'nunique'],\n",
    "        'second' : ['max', 'min', 'nunique', np.ptp],\n",
    "    }\n",
    "\n",
    "    feature = group_func(df, feature_type, group_key=['store_id'])\n",
    "    feature = add_prefix(feature, ['store_id'], 'store_' + name + '_')\n",
    "    feature.to_hdf('../feature/recall/store_feature', name)\n",
    "    print('>>> store feature success')\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_second_diff_feature(df):\n",
    "    \"\"\"\n",
    "    用户时间间隔统计特征：\n",
    "    聚合层级：cate_id, store_id, item_id\n",
    "    \n",
    "    1. 商品与下个商品间隔\n",
    "    2. 商品与下个同样商品间隔\n",
    "    3. 商品与下个同品类商品间隔\n",
    "    4. 商品与下个同店铺商品间隔\n",
    "    \n",
    "    备注：线下：0.8843→0.8852  提升：0.009\n",
    "    ---------------------------------------------\n",
    "    \n",
    "    \"\"\"\n",
    "    df = df[['buyer_admin_id', 'store_id', 'cate_id', 'item_id', 'second']].drop_duplicates()\n",
    "    df['second_diff'] = df['second'] - df.groupby(['buyer_admin_id'])['second'].shift(1)\n",
    "    df['cate_id_second_diff'] = df['second'] - df.groupby(['buyer_admin_id', 'cate_id'])['second'].shift(1)\n",
    "    df['store_id_second_diff'] = df['second'] - df.groupby(['buyer_admin_id', 'store_id'])['second'].shift(1)\n",
    "    \n",
    "    feature_type = {\n",
    "        'second_diff' : ['max', 'min', 'mean', 'std', np.ptp],\n",
    "        'cate_id_second_diff':['max', 'min', 'mean', 'std', np.ptp],\n",
    "        'store_id_second_diff':['max', 'min', 'mean', 'std', np.ptp],\n",
    "    }\n",
    "    \n",
    "    df = filter_sample(df)\n",
    "    feature = group_func(df, feature_type, group_key=['buyer_admin_id'])\n",
    "    feature = add_prefix(feature, ['buyer_admin_id'], 'user_second_diff_')\n",
    "    feature.to_hdf('../feature/recall/user_second_diff_feature', 'user')\n",
    "    \n",
    "    for level in ['cate_id', 'store_id']:\n",
    "        feature = group_func(df, feature_type, group_key=['buyer_admin_id', level])\n",
    "        feature = add_prefix(feature, ['buyer_admin_id', level], 'user_' + level + '_second_diff_')\n",
    "        feature.to_hdf('../feature/recall/user_second_diff_feature', level)\n",
    "    print('>>> user_second_diff_feature success')\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_item_conv_feature(df):\n",
    "    \"\"\"\n",
    "    商品转化率特征\n",
    "    \n",
    "    \"\"\"\n",
    "    item_pv = df.drop_duplicates(subset=['buyer_admin_id', 'item_id', 'second']).groupby(['item_id']).size().to_frame('pv').reset_index()\n",
    "    item_uv = df.groupby(['item_id'])['buyer_admin_id'].nunique().to_frame('uv').reset_index()\n",
    "    item_buy_uv = df[df['buy_flag']==1].groupby(['item_id'])['buyer_admin_id'].nunique().to_frame('buy_uv').reset_index()\n",
    "\n",
    "    dup = df[df['buy_flag']==1][df.duplicated(subset=['buyer_admin_id', 'item_id', 'second'], keep=False)]\n",
    "    multi_buy_uv = dup.groupby(['item_id'])['buyer_admin_id'].nunique().to_frame('multi_buy_uv').reset_index()\n",
    "\n",
    "    view_time = df.groupby(['buyer_admin_id', 'item_id']).size().to_frame('user_view_time').reset_index()\n",
    "    view_one_time = view_time.groupby(['item_id'])['user_view_time'].value_counts(normalize=True).to_frame('view_onetime_prop').reset_index()\n",
    "    view_one_time = view_one_time[view_one_time['user_view_time']==1].drop(['user_view_time'],1 )\n",
    "    \n",
    "    last = df.drop_duplicates(subset=['buyer_admin_id'], keep='first')\n",
    "    last_cnt = last.groupby(['item_id']).size().to_frame('last_buy').reset_index()\n",
    "    \n",
    "    last_via_day = df.drop_duplicates(subset=['buyer_admin_id', 'day'], keep='first')\\\n",
    "        .drop_duplicates(subset=['buyer_admin_id', 'item_id'], keep='first')\n",
    "    last_via_day_cnt = last_via_day.groupby(['item_id']).size().to_frame('last_buy_day').reset_index()\n",
    "    \n",
    "    \n",
    "    feature = item_pv.merge(item_uv, on=['item_id'], how='left')\\\n",
    "            .merge(item_buy_uv, on=['item_id'], how='left')\\\n",
    "            .merge(multi_buy_uv, on=['item_id'], how='left')\\\n",
    "            .merge(view_one_time, on=['item_id'], how='left')\\\n",
    "            .merge(last_cnt, on=['item_id'], how='left')\\\n",
    "            .merge(last_via_day_cnt, on=['item_id'], how='left').fillna(0)\n",
    "\n",
    "    feature['pv/uv'] = feature['pv'] / feature['uv']\n",
    "    feature['buy_uv/pv'] = feature['buy_uv'] / feature['uv']\n",
    "    feature['multi_buy_uv/buy_uv'] = feature['multi_buy_uv'] / feature['buy_uv']\n",
    "    feature['multi_buy_uv/uv'] = feature['multi_buy_uv'] / feature['uv']\n",
    "    feature['last_buy/uv'] = feature['last_buy'] / feature['uv']\n",
    "    feature['last_buy/buy_uv'] = feature['last_buy'] / feature['buy_uv']\n",
    "    feature = feature.fillna(0)\n",
    "    \n",
    "    feature = add_prefix(feature, ['item_id'], 'item_conv_')\n",
    "    feature.to_hdf('../feature/recall/item_conv_feature', 'all')\n",
    "    print('>>> item_conv_feature success')\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_rank_feature(feature, feature_name, key, group_key=[], ascending=False):\n",
    "    \"\"\"\n",
    "    import pandas\n",
    "    name = 'user_item_dedup_feature'\n",
    "    feat = pandas.read_hdf('../feature/rank/' + name, 'all')\n",
    "    item = pandas.read_csv('../data/Antai_AE_round2_item_attr_20190813.zip')[['item_id', 'cate_id', 'store_id']]\n",
    "    feature = pandas.merge(feat, item, how='left', on=['item_id'])\n",
    "\n",
    "    get_user_rank_feature(feature, name, key=['item_id'], group_key=[] ,ascending=True)\n",
    "    get_user_rank_feature(feature, name, key=['item_id'], group_key=[], ascending=False)\n",
    "    get_user_rank_feature(feature, name, key=['item_id'], group_key=['cate_id'] ,ascending=True)\n",
    "    get_user_rank_feature(feature, name, key=['item_id'], group_key=['cate_id'], ascending=False)\n",
    "    get_user_rank_feature(feature, name, key=['item_id'], group_key=['store_id'] ,ascending=True)\n",
    "    get_user_rank_feature(feature, name, key=['item_id'], group_key=['store_id'], ascending=False)\n",
    "\n",
    "    name = 'user_cate_dedup_feature'\n",
    "    feature = pandas.read_hdf('../feature/rank/' + name, 'all')\n",
    "    get_user_rank_feature(feature, name, key=['cate_id'], group_key=[] ,ascending=True)\n",
    "    get_user_rank_feature(feature, name, key=['cate_id'], group_key=[], ascending=False)\n",
    "\n",
    "    name = 'user_store_dedup_feature'\n",
    "    feature = pandas.read_hdf('../feature/rank/' + name, 'all')\n",
    "    get_user_rank_feature(feature, name, key=['store_id'], group_key=[] ,ascending=True)\n",
    "    get_user_rank_feature(feature, name, key=['store_id'], group_key=[], ascending=False)\n",
    "    \n",
    "    name = 'user_second_diff_feature'\n",
    "    feature = pandas.read_hdf('../feature/rank/' + name, 'user')\n",
    "    get_user_rank_feature(feature, name, key=[], group_key=[] ,ascending=True)\n",
    "    get_user_rank_feature(feature, name, key=[], group_key=[], ascending=False)    \n",
    "\n",
    "    name = 'user_second_diff_feature'\n",
    "    feature = pandas.read_hdf('../feature/rank/' + name, 'item_id')\n",
    "    get_user_rank_feature(feature, name, key=[], group_key=[] ,ascending=True)\n",
    "    get_user_rank_feature(feature, name, key=[], group_key=[], ascending=False)\n",
    "    \n",
    "    name = 'user_second_diff_feature'\n",
    "    feature = pd.read_hdf('../feature/rank/' + name, 'item_id')\n",
    "    item = pd.read_csv('../data/Antai_AE_round2_item_attr_20190813.zip')[['item_id', 'cate_id', 'store_id']]\n",
    "    feature = pd.merge(feature, item, how='left', on=['item_id'])\n",
    "\n",
    "    import modin.pandas\n",
    "    get_user_rank_feature(feature.copy(), name, key=['item_id'], group_key=[] ,ascending=True)\n",
    "    get_user_rank_feature(feature.copy(), name, key=['item_id'], group_key=[], ascending=False)\n",
    "    get_user_rank_feature(feature.copy(), name, key=['item_id'], group_key=['cate_id'] ,ascending=True)\n",
    "    get_user_rank_feature(feature.copy(), name, key=['item_id'], group_key=['cate_id'], ascending=False)\n",
    "    get_user_rank_feature(feature.copy(), name, key=['item_id'], group_key=['store_id'] ,ascending=True)\n",
    "    get_user_rank_feature(feature.copy(), name, key=['item_id'], group_key=['store_id'], ascending=False)\n",
    "    \n",
    "    name = 'user_item_lastday_dedup_feature'\n",
    "    feature = pd.read_hdf('../feature/rank/' + name, 'all')\n",
    "    item = pd.read_csv('../data/Antai_AE_round2_item_attr_20190813.zip')[['item_id', 'cate_id', 'store_id']]\n",
    "    feature = pd.merge(feature, item, how='left', on=['item_id'])\n",
    "\n",
    "    import modin.pandas\n",
    "    get_user_rank_feature(feature.copy(), name, key=['item_id'], group_key=[] ,ascending=True)\n",
    "    get_user_rank_feature(feature.copy(), name, key=['item_id'], group_key=[], ascending=False)\n",
    "    get_user_rank_feature(feature.copy(), name, key=['item_id'], group_key=['cate_id'] ,ascending=True)\n",
    "    get_user_rank_feature(feature.copy(), name, key=['item_id'], group_key=['cate_id'], ascending=False)\n",
    "    get_user_rank_feature(feature.copy(), name, key=['item_id'], group_key=['store_id'] ,ascending=True)\n",
    "    get_user_rank_feature(feature.copy(), name, key=['item_id'], group_key=['store_id'], ascending=False)\n",
    "    ------------------------------------------------------------------------------------------\n",
    "    'user_item_dedup_feature', 'asc'\n",
    "    'user_item_dedup_feature', 'desc'\n",
    "    'user_item_dedup_feature', 'cate_id_asc'\n",
    "    'user_item_dedup_feature', 'cate_id_desc\n",
    "    'user_item_dedup_feature', 'store_id_asc'\n",
    "    'user_item_dedup_feature', 'store_id_desc\n",
    "    \n",
    "    'user_cate_dedup_feature', 'asc'\n",
    "    'user_cate_dedup_feature', 'desc'    \n",
    "\n",
    "    'user_store_dedup_feature', 'asc'\n",
    "    'user_store_dedup_feature', 'desc'\n",
    "    \n",
    "    'user_second_diff_feature', 'asc'\n",
    "    'user_second_diff_feature', 'desc'\n",
    "    \n",
    "    'user_second_diff_feature', 'cate_id_asc'\n",
    "    'user_second_diff_feature', 'cate_id_desc'\n",
    "    \n",
    "    'user_second_diff_feature', 'store_id_asc'\n",
    "    'user_second_diff_feature', 'store_id_desc'\n",
    "    \n",
    "    'user_item_lastday_dedup_feature', 'asc'\n",
    "    'user_item_lastday_dedup_feature', 'desc'\n",
    "    \n",
    "    'user_item_lastday_dedup_feature', 'cate_id_asc'\n",
    "    'user_item_lastday_dedup_feature', 'cate_id_desc'\n",
    "    \n",
    "    'user_item_lastday_dedup_feature', 'store_id_asc'\n",
    "    'user_item_lastday_dedup_feature', 'store_id_desc'\n",
    "    \n",
    "    用户 * 商品 * 排序 基础特征：\n",
    "    \n",
    "    备注：\n",
    "    1. desc降序：线下：0.8795→0.8795  提升:0\n",
    "    1. asc升序：线下：0.8795→0.8813  提升:-0.0018\n",
    "    ---------------------------------------------\n",
    "    \"\"\"\n",
    "    if ascending:\n",
    "        name = 'asc'\n",
    "    else:\n",
    "        name = 'desc'\n",
    "    columns = []\n",
    "    for col in feature.columns:\n",
    "        if col not in ['buyer_admin_id', 'item_id', 'cate_id', 'store_id']:\n",
    "            column_name = col + '_rank_' + name\n",
    "            feature[column_name] = feature.groupby(['buyer_admin_id'] + group_key)[col].rank(ascending=ascending, method='dense')\n",
    "            columns.append(column_name)\n",
    "            \n",
    "    if len(group_key)>0:\n",
    "        feature = feature[['buyer_admin_id', 'item_id'] + group_key + columns]\n",
    "        name = group_key[0] + '_' + name\n",
    "    else:\n",
    "        feature = feature[['buyer_admin_id']+ key + columns]\n",
    "        \n",
    "    feature.to_hdf('../feature/recall/' + feature_name, name)\n",
    "    print('>>> user_rank_feature feature success')\n",
    "    return feature"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (PySpark)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
